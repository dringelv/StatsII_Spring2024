toks_col <- tokens_remove(tokens(toks), "")
toks_col <- tokens_remove(tokens(toks), c("n", "get", "go", "one", "just", "nthe", "nthis", "nenviron", "npaid", "n n nwe", "n nthe", "n ni", "thing"))
# Example document after converting collocations using an unsupervised method
toks_col[[1]][1:50] # show first 50 tokens
dfm <- dfm(toks_col)
saveRDS(dfm, file = "dfm_file.rds")
# Your code here
toks <- tokens(corpus,
include_docvars = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
#dfm <- # your final object should be called dfm
dfm <- dfm(toks)
dfm <- dfm_trim(dfm, min_termfreq = 30)
dfm <- dfm_tfidf(dfm) # weight DFM
topfeatures(dfm)
stop_list <- stopwords("english") # load English stopwords from quanteda
toks <- tokens_remove(toks, stop_list)
# stemming
toks <- tokens_wordstem(toks)
toks[[6]][1:50]
#Compounding
library(quanteda.textstats)
unsup_col <- textstat_collocations(toks,
method = "lambda",
size = 2,
min_count = 10,
smoothing = 0.5)
unsup_col <- unsup_col[order(-unsup_col$count),] # sort detected collocations by count (descending)
head(unsup_col, 25)
toks_col <- tokens_compound(toks,
pattern = unsup_col[unsup_col$z > 3,])
toks_col <- tokens_remove(tokens(toks), "")
toks_col <- tokens_remove(tokens(toks), c("n", "get", "go", "one", "just", "nthe", "nthis", "nenviron", "npaid", "n n nwe", "n nthe", "n ni", "thing"))
saveRDS(dfm, file = "dfm_file.rds")
dfm <- readRDS("dfm_file.rds")
top_features <- topfeatures(dfm, n = 10)
print(top_features)
ntoken(dfm)
# Your code here
toks <- tokens(corpus,
include_docvars = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
stop_list <- stopwords("english") # load English stopwords from quanteda
toks <- tokens_remove(toks, stop_list)
# stemming
toks <- tokens_wordstem(toks)
#dfm <- # your final object should be called dfm
dfm <- dfm(toks)
dfm <- dfm_trim(dfm, min_termfreq = 30)
dfm <- dfm_tfidf(dfm) # weight DFM
topfeatures(dfm)
toks[[6]][1:50]
# Your code here
toks <- tokens(corpus,
include_docvars = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
stop_list <- stopwords("english") # load English stopwords from quanteda
toks <- tokens_remove(toks, stop_list, "n")
# Your code here
toks <- tokens(corpus,
include_docvars = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
stop_list <- stopwords("english") # load English stopwords from quanteda
toks <- tokens_remove(toks, c(stop_list, "n"))
# stemming
toks <- tokens_wordstem(toks)
#dfm <- # your final object should be called dfm
dfm <- dfm(toks)
dfm <- dfm_trim(dfm, min_termfreq = 30)
dfm <- dfm_tfidf(dfm) # weight DFM
topfeatures(dfm)
toks[[6]][1:50]
saveRDS(dfm, file = "dfm_file.rds")
dfm <- readRDS("dfm_file.rds")
top_features <- topfeatures(dfm, n = 10)
print(top_features)
ntoken(dfm)
top_features <- topfeatures(dfm, n = 10)
print(top_features)
library(caret)
tmpdata <- convert(dfm, to = "data.frame")# convert the dfm object to a data.frame
tmpdata <- tmpdata[, -1] # drop document id variable (first variable)
sentiment_labels <- docvars(dfm, "sentiment")
sentiment <- as.factor(sentiment_labels) # extract sentiment labels from the dfm object (hint: the dfm is an S4 class object)
ldata <- cbind(tmpdata, sentiment)# bind sentiment and tmpdata to create a labelled data frame
train_row_nums <- createDataPartition(y = sentiment, # set sentiment as the Y variable in caret
p = 0.8, # fill in the blank
list=FALSE)
Train <- ldata[train_row_nums, ]# use train_row_nums to subset ldata and create the training set
Test <- ldata[-train_row_nums, ] # train_row_nums to subset ldata and create the testing set
library('doParallel') # for parallel processing
library('naivebayes') # naive bayes classifier
library('MLmetrics') # model performance
# 1. Set grid search for NB classifier - how will you tune your parameters?
modelLookup(model = "naive_bayes")
tgrid <- expand.grid(laplace = c(0,0.5,1.0),
usekernel = c(TRUE, FALSE),
adjust=c(0.75, 1, 1.25, 1.5))# your code here
# 2. Set up 5-fold cross-validation, repeated 3 times
train_control <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 3,
classProbs= TRUE,
summaryFunction = multiClassSummary,
selectionFunction = "best", # select the model with the best performance metric
verboseIter = TRUE)
# 3. Set parallel processing cluster
cl <- makePSOCKcluster(5) # create number of copies of R to run in parallel and communicate over sockets
registerDoParallel(cl) # register parallel backed with foreach package
# 4. Train model
nb_train <- train(
sentiment ~ ., # fill in formula here
data = Train,
method = "naive_bayes", # fill in method
metric = "F1", # fill in metric to optimise
trControl = train_control,
tuneGrid = tgrid,
allowParallel= TRUE
)
stopCluster(cl) # stop parallel process once job is done
print(nb_train) # print cross-validation results
saveRDS(nb_train, "data/nb_train")
saveRDS(nb_train, "data/nb_train")
saveRDS(nb_train, "nb_train")
pred <- predict(nb_train, newdata = Test)# Your code here
pred <- predict(nb_train, newdata = Test)# Your code here
head(pred) # See first few predictions
# generate a confusion matrix
confusionMatrix(reference = as.factor(Test$sentiment), data = pred, mode='everything') # generate confusion matrix
# explore the most predictive features
var_imp <- varImp(object=nb_train) # calculate feature importance
plot(var_imp, top = 20) # plot top 20 most important features
plot(var_imp, top = 20) # plot top 20 most important features
plot(var_imp, top = 20)
# generate a confusion matrix
confusionMatrix(reference = as.factor(Test$sentiment), data = pred, mode='everything') # generate confusion matrix
# explore the most predictive features
var_imp <- varImp(object=nb_train) # calculate feature importance
plot(var_imp, top = 20) # plot top 20 most important features
# explore key words in context
keys_of_interest <- list(var_imp) # add keywords to this list
for(i in keys_of_interest){
kwic_word <- kwic(corpus, i, window = 15)
print(head(kwic_word))
}
# generate a confusion matrix
confusionMatrix(reference = as.factor(Test$sentiment), data = pred, mode='everything') # generate confusion matrix
# explore the most predictive features
var_imp <- varImp(object=nb_train) # calculate feature importance
plot(var_imp, top = 20) # plot top 20 most important features
# explore key words in context
keys_of_interest <- list(var_imp[1:20]) # add keywords to this list
for(i in keys_of_interest){
kwic_word <- kwic(corpus, i, window = 15)
print(head(kwic_word))
}
print(var_imp) # plot top 20 most important features
# generate a confusion matrix
confusionMatrix(reference = as.factor(Test$sentiment), data = pred, mode='everything') # generate confusion matrix
# explore the most predictive features
var_imp <- varImp(object=nb_train) # calculate feature importance
plot(var_imp, top = 20)
print(var_imp) # plot top 20 most important features
# explore key words in context
keys_of_interest <- list("great", "love", "ask", "delici", "said", "order",	"never",	"told", "minut","go") # add keywords to this list
for(i in keys_of_interest){
kwic_word <- kwic(corpus, i, window = 15)
print(head(kwic_word))
}
library(kernlab)
tgrid <- expand.grid(C = c(0.5, 1, 1.5))# set your parameters for tuning here (note: depending on how you set these, your model may take significantly longer to run)
# set cross-validation (5-fold without repeats)
train_control <- trainControl(
method = "cv",
number = 5,
classProbs= TRUE,
summaryFunction = multiClassSummary,
selectionFunction = "best",
verboseIter = TRUE
)
# set parallel processing cluster
cl <- makePSOCKcluster(5) # create number of copies of R to run in parallel and communicate over sockets
registerDoParallel(cl) # register parallel backed with foreach package
# train model
svm_train <- train(
sentiment ~ .,
data = Train,
method = "svmLinear",
metric = "F1",
trControl = train_control,
tuneGrid = tgrid,
allowParallel= TRUE
)
# stop cluster
stopCluster(cl)
# print results
print(svm_train)
# save results
saveRDS(svm_train, "svm_train.RDS")
svm_train <- readRDS("svm_train.RDS")
pred_svm <- predict(svm_train, newdata = Test)# Your code here
head(pred_svm)
# generate a confusion matrix
confusionMatrix(reference = as.factor(Test$sentiment), data = pred_svm, mode='everything') # generate confusion matrix
# explore the most predictive features
var_imp <- varImp(object=svm_train) # calculate feature importance
plot(var_imp, top = 20)
print(var_imp) # plot top 20 most important features
remotes::install_github("news-r/newsapi")
install.packages("remotes")
install.packages("remotes")
newsapi_key("8a9497de33b44e27a32da334b4e3f615")
library(newsapi)
newsapi_key("8a9497de33b44e27a32da334b4e3f615")
library(newsapi)
library(rvest)
library(rvest)
url <- https://www.lrt.lt/naujienos/lietuvoje
url <- https:/www.lrt.lt/naujienos/lietuvoje
url <- "https://www.lrt.lt/naujienos/lietuvoje"
webpage <- read_html(url)
View(webpage)
news_titles <- webpage %>%
html_nodes(".news-title") %>%   # Replace ".news-title" with the appropriate CSS selector for the news titles
html_text()
news_links <- webpage %>%
html_nodes(".news-title a") %>%  # Replace ".news-title a" with the appropriate CSS selector for the links within the news titles
html_attr("href")                # Extract the href attribute (URL) of the links
# Print the scraped news titles and links
print(news_titles)
print(news_links)
news_titles <- webpage %>%
html_nodes(".news__title") %>%   # Replace ".news-title" with the appropriate CSS selector for the news titles
html_text()
news_links <- webpage %>%
html_nodes(".news__title a") %>%  # Replace ".news-title a" with the appropriate CSS selector for the links within the news titles
html_attr("href")                # Extract the href attribute (URL) of the links
# Print the scraped news titles and links
print(news_titles)
print(news_links)
url <- "https://www.lrt.lt/paieska?q=lgbt"
webpage <- read_html(url)
news_titles <- webpage %>%
html_nodes(".news__title") %>%   # Replace ".news-title" with the appropriate CSS selector for the news titles
html_text()
news_links <- webpage %>%
html_nodes(".news__title a") %>%  # Replace ".news-title a" with the appropriate CSS selector for the links within the news titles
html_attr("href")                # Extract the href attribute (URL) of the links
news_titles <- webpage %>%
html_nodes(".news__title") %>%   # Replace ".news-title" with the appropriate CSS selector for the news titles
html_text()
news_links <- webpage %>%
html_nodes(".news__title a") %>%  # Replace ".news-title a" with the appropriate CSS selector for the links within the news titles
html_attr("href")                # Extract the href attribute (URL) of the links
# Print the scraped news titles and links
print(news_titles)
library(tidyverse) # load our packages here
library(xml2)
library(xml2)
library(rvest)
xml_structure(html)
xml_structure(webpage)
capture.output(xml_structure(webpage))
html <- read_html(url)
html <- read_html(url)
xml_structure(html)
capture.output(xml_structure(html))
news_titles <- html %>%
html_nodes(".news__title") %>%   # Replace ".news-title" with the appropriate CSS selector for the news titles
html_text()
news_links <- html %>%
html_nodes(".news__title a") %>%  # Replace ".news-title a" with the appropriate CSS selector for the links within the news titles
html_attr("href")                # Extract the href attribute (URL) of the links
html %>%
html_nodes(xpath = ".news__title")
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
long_data <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Long.txt", header=T)
library(tidyverse)
library(ggplot2)
long_data <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Long.txt", header=T)
# wrangle
long_data <- within(long_data, {
fem <- as.logical(fem)
mar <- as.logical(mar)
})
# EDA
str(long_data)
summary(long_data)
with(long_data,
list(mean(art), var(art))) # do we meet assumptions for Poisson?
# a) Examine distribution of response variable
hist(long_data$art)
ggplot(long_data, aes(ment, art, color = fem)) +
geom_jitter(alpha = 0.5)
# OLS?
mod.lm <- lm(art ~ ., data = long_data)
summary(mod.lm)
mod2.lm <- lm(art ~ fem * ., data = long_data) # interaction effects with gender?
summary(mod2.lm)
# Do we meet assumptions?
plot(predict(mod2.lm), abs(resid(mod2.lm)), xlab = "Predicted", ylab = "Absolute Residuals")
sresid <- rstandard(mod2.lm) # distribution of standardised residuals
hist(sresid, main = "")
par(mfrow = c(2, 2))
plot(mod2.lm) # R's built-in plots for linear regression model assessment
# b) Poisson regression
mod.ps <- glm(art ~ ., data = long_data, family = poisson)
summary(mod.ps)
# interpreting outputs
cfs <- coef(mod.ps)
# predicted no. of articles for a married male PhD researcher with 1 child at 2-rated
# institute whose PhD supervisor published 5 articles.
exp(cfs[1] + cfs[2]*0 + cfs[3]*5 + cfs[4]*2 + cfs[5]*1 + cfs[6]*1)
pred <- data.frame(fem = FALSE,
ment = 5,
phd = 2,
mar = TRUE,
kid5 = 1)
# check with predict() function
predict(mod.ps, newdata = pred, type = "response")
# plot predictions vs count
ggplot(data = NULL, aes(x = mod.ps$fitted.values, y = long_data$art)) +
geom_jitter(alpha = 0.5) +
geom_abline(color = "blue") #+
# calculate pseudo R squared
1 - (mod.ps$deviance/mod.ps$null.deviance)
# calculate RMSE
sqrt(mean((mod.ps$model$art - mod.ps$fitted.values)^2))
# Add an interaction?
mod2.ps <- glm(art ~ fem * ., data = long_data, family = poisson)
summary(mod2.ps)
1 - (mod2.ps$deviance/mod2.ps$null.deviance) # pseudo R2
sqrt(mean((mod2.ps$model$art - mod2.ps$fitted.values)^2)) # RMSE
# c) Over-dispersion?
install.packages("AER")
library(AER)
dispersiontest(mod.ps)
install.packages("pscl")
library(pscl)
mod.zip <- zeroinfl(art ~ ., data = long_data, dist = "poisson")
summary(mod.zip)
View(long_data)
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("nnet", "MASS"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data
gdp_data <- read.csv("https://raw.githubusercontent.com/ASDS-TCD/StatsII_Spring2024/main/datasets/gdpChange.csv", stringsAsFactors = F)
View(gdp_data)
# import data
diocese_data <− read.csv ( "https://raw.githubusercontent.com/ASDS−TCD/StatsII_Spring2024/main/datasets/diocese_data.csv", stringsAsFactors = F )
# import data
diocese_data <− read.csv("https://raw.githubusercontent.com/ASDS−TCD/StatsII_Spring2024/main/datasets/diocese_data.csv", stringsAsFactors = F )
# import data
diocese_data <− read.csv("https://raw.githubusercontent.com/ASDS−TCD/StatsII_Spring2024/main/datasets/diocese_data.csv", stringsAsFactors = F )
summary(gdp_data)
multinom_model1 <- multinom(GDPdiff ~ REG + OIL, data = gdp_data)
multinom_model1 <- multinom(GDPWdiff ~ REG + OIL, data = gdp_data)
gdp_data$GDPWdiff <- relevel(gdp_data$GDPWdiff, ref = "no change")
breaks <- c(-Inf, 0, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
breaks <- c(-Inf, 0, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
# Print first few rows to verify
head(data)
# Print first few rows to verify
head(gdp_data)
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(gdp_data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
breaks <- c(-Inf, 0, 0, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(gdp_data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
breaks <- c(-Inf, 0, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(gdp_data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
# Print first few rows to verify
head(gdp_data)
breaks <- c(-Inf, 0, 0.000001, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(gdp_data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
# Print first few rows to verify
head(gdp_data)
breaks <- c(-Inf, -0.000001, 0, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(gdp_data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
# Print first few rows to verify
head(gdp_data)
gdp_data$GDPWdiff <- relevel(gdp_data$GDPWdiff, ref = "no change")
gdp_data$GDPWdiff <- relevel(gdp_data$GDPWdiff_cat, ref = "no change")
multinom_model1 <- multinom(GDPWdiff_cat ~ REG + OIL, data = gdp_data)
summary(mult.log)
exp(coef(mult.log))
summary(multinom_model1)
exp(coef(multinom_model1))
# load data
gdp_data <- read.csv("https://raw.githubusercontent.com/ASDS-TCD/StatsII_Spring2024/main/datasets/gdpChange.csv", stringsAsFactors = F)
summary(gdp_data)
breaks <- c(-Inf, -0.000001, 0, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(gdp_data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
# Print first few rows to verify
head(gdp_data)
gdp_data$GDPWdiff_cat <- relevel(gdp_data$GDPWdiff_cat, ref = "no change")
multinom_model1 <- multinom(GDPWdiff_cat ~ REG + OIL, data = gdp_data)
summary(multinom_model1)
exp(coef(multinom_model1))
# load data
gdp_data <- read.csv("https://raw.githubusercontent.com/ASDS-TCD/StatsII_Spring2024/main/datasets/gdpChange.csv", stringsAsFactors = F)
summary(gdp_data)
breaks <- c(-Inf, -0.000001, 0, Inf)  # We're defining two intervals: (-Inf, 0] and (0, Inf)
# Define labels for the intervals
labels <- c("negative","no change", "positive")
# Create a new column with categorical data
gdp_data$GDPWdiff_cat <- cut(gdp_data$GDPWdiff, breaks = breaks, labels = labels, include.lowest = TRUE)
# Print first few rows to verify
head(gdp_data)
multinom_model1 <- multinom(GDPWdiff_cat ~ REG + OIL, data = gdp_data)
summary(multinom_model1)
exp(coef(multinom_model1))
gdp_data$GDPWdiff_cat <- relevel(gdp_data$GDPWdiff_cat, ref = "no change")
multinom_model1 <- multinom(GDPWdiff_cat ~ REG + OIL, data = gdp_data)
summary(multinom_model1)
gdp_data$GDPWdiff_cat <- relevel(gdp_data$GDPWdiff_cat, ref = "no change")
multinom_model1 <- multinom(GDPWdiff_cat ~ REG + OIL, data = gdp_data)
summary(multinom_model1)
exp(coef(multinom_model1))
gdp_data$GDPWdiff_cat <- relevel(gdp_data$GDPWdiff_cat, ref = "no change")
multinom_model1 <- multinom(GDPWdiff_cat ~ REG + OIL, data = gdp_data)
summary(multinom_model1)
exp(coef(multinom_model1))
polr_model1 <- polr(GDPWdiff_cat ~ REG + OIL, data = gdp_data, Hess= TRUE)
summary(polr_model1)
exp(coef(polr_model1))
# load data
mexico_elections <- read.csv("https://raw.githubusercontent.com/ASDS-TCD/StatsII_Spring2024/main/datasets/MexicoMuniData.csv")
View(mexico_elections)
View(mexico_elections)
poisson_model1 <- glm(PAN.visits.06 ~ competitive.district + marginality.06 + PAN.governor.06, data = mexico_elections, family = poisson)
summary(poisson_model1)
